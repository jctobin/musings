As i read this article https://stackoverflow.blog/2019/11/14/why-is-the-migration-to-python-3-taking-so-long/?cb=1 i realized that i should start to layout all the changes that i helped bring to at least the start of fruition at highmark. When i started in 2016 i was brought on to help setup a "legal litigation library"; in simple terms, a place where you could dump all the data that wasn't actually being used anymore, but you had to keep around in its exact state for legal reasons. i got to work alongside a consulting company who were there to train myself and a couple other people on the use of Hadoop. Having worked with hive previously, i had a bit of a head-start, but in all honesty, my previous work had been removing the necessity for hadoop (processing 3GB of data daily does not require hadoop, a point i'll come back to later). anyway, these consultants had this 'super great tool that automatically does everything' to move tables from an rdbms into hive/hadoop (external hive tables). im not sure if it was our inability or the tools, but our team of internal workers could never get their product to work correctly; when we asked for a demo from our on-site consultants they were also never able to get it to work in front of us. after requesting the original java code and pouring through its thousands upon thousands of lines of code, I got the gist of how it was doing what. having been reading about all the new, amazing things in the hadoop ecosystem at the time, i was intrigued with spark so i started writing a similar 'ingestion framework' for spark. i had never written scala, and had only written java for one class in college; the consultants told me "you dont need to write that, our framework is great, and you dont know java. you have no chance of writing something this complicated". they were right about one thing, i did not write something that complicated. i started writing code on a wednesday and by midday on friday i had a working prototype. it did everything these consultants framework did, but mine was able to be used by the non-hadoop-experts on our team, it moved the data over faster, and it didnt run into some of the data-type conversion problems that sqoop could run into at the time. needless to say, the remaining 2 weeks of these consultants time with us was awkward. in a language i had never used, with a platform(spark) i had just learned, in an enterprise i had just started working at 2 weeks prior, i had built a usable framework that had taken the consulting company of 'experts' Months to create. i dont say this to toot my own horn, but hopefully to begin to demonstrate how i work on problems. my framework was about 200 lines of code (its still in use by multiple departments, but has been expanded to closer to 500 as additional features were added). it didnt need to be some crazy complicated thing, people much smarter than i, and developers much better than i did almost all the work. have you ever checked into the source code for spark.dataframe.jdbc (https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/jdbc/)? if you've never seen scala it can look a little daunting, but they did all the work pretty much. but you cant even say that they did the work, they just built on the java jdbc standard, which is used in pretty much every enterprise in the world. Just check out the OracleDialect for utilizing spark+jdbc+sql (https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/jdbc/OracleDialect.scala) if we remove the comments/documentation from that code, its what, 60 lines of code? sure it imports other things and is built modularly, but 60 lines of code to be able to access/use oracle with spark. the mysql one is even shorter, with comments its 51 lines. having written/adapted internal rdbms dialects for spark.jdbc i can say that that's all it takes to include a new source, 50-100 lines of 'new' code. 'new' because most of it is the same as another rdbms, you just have to take rdbms specifications into account, teradata allows you to do 'select top 10 *' and has VARCHAR, hive you'd probably just use `string` and would say `select * from table limit 10`. same stuff, slightly different syntax. as one of my former bosses has in his signature, quoting E.F. Schumacher: 
 > Any intelligent fool can make things bigger, more complex, and more violent. It takes a touch of genius â€” and a lot of courage to move in the opposite direction.
 
 maybe i've been blessed with a touch of genius. in my mind, it was just easier to look at a couple files and a couple hundred lines of code compared with hundreds of files and thousands upon thousands of lines of code. easier to look at, easier to test, to maintain, to teach; those are all desireable things. as i shifted to another team, i was able to hand off my 200 lines of code to someone who had never seen scala before, but within a 1 hour training session they were able to operate the framework all on their own. they might not know all the intricacies of scala, or exactly what's going on under the hood, but looking at something that looks like:
 ``` scala
 val tablenames = args[0].split(",")
 for (table <- tablenames){
    grab_table_from_source(table)
    if (need_to_alter==true){
        alter_table(table, params=etc)
    write_table_to_hive(table)
 }
```
that's Pretty simple to understand, regardless of your scala knowledge, or even really your coding ability. 
so there's story number1. i was the first person at highmark to write scala and/or have scala go into production. now there are at least a dozen people who have had that opportunity. i saved the company $500k which is what the consulting company wanted us to spend on their 'amazing framework'. but perhaps most importantly with that project was that i could hand it off to a person with no hadoop nor scala experience and they were able to continue migrating thousands of tables. 

the lack of hadoop experience, or even lack of open source technologies experience was also something that i got to work through at the enterprise. when i started my project was the first project to utilize the hadoop clusters. i got to work directly with the 'IT' team to setup parameters, confer on security settings, really dig deep into a lot of the things that kinda just come about with a new platform. there were a number of hiccups and stumbling blocks as there is apt to be with new things, but that ecosystem exploded (in a good way) during my tenure at highmark. i was one of the first 10 people on the platform. i wrote up tutorials on how to do all kinds of things in spark in our ecosystem. most of them were similar to what was available on the web, but each enterprise does have its own nuances so my 20ish how-tos were read by hundreds of people. when i left highmark the number of people working in the hadoop ecosystem was at least 200. in retrospect, my frustrations at having to explain the same things for years are most definitely a fault of my own. as the article that started this thought train stated, enterprises move slowly, *especially* heavily regulated ones, like healthcare. in my mind, i had seen the speed and ease at which i was able to get that first project rolling and thought 'this is how all projects can/will be', but that was one where i could do everything. in the words of another one of my former boss's "you are very technically skilled james, but you wont be able to achieve the maximum you can until you can get a team to execute on the plan you set forth." it worded it much better than i just did, but i got/remember the gist. i've tried to document and provide training materials for almost everything i've done, but that's a sort of leading from the sideline. putting the tools in people hands for them to operate on their own. that has its place, and is how i enjoy my managers operating, in a sort of "just give me a direction and the freedom to get there"; ive been realizing that that is not how most people ideally operate. notably in a world with nearly unlimited choices, simply choosing the 'right tool' for many people can be daunting. should i write/learn python? R? scala? which is the best? which platform should i use, which tools do i need, what additional ones should i buy?!? the open source community is incredibly large at this point, with SO many tools available at the click of a download button. this was another area that i got to be part of at highmark. 
i was part of an open source steering committee within my first 6 months of being at highmark. this was a committee with members from a lot of the organizations , most of whom were doing data science stuff, or at least analytics in some way. i have an analytic background, but definitely consider myself more of a bridge between analytics and IT , than an analytics person. anyway, in this group we put together use-cases for using python and r, because we had to pitch using them. as in, they did not exist in the enterprise at an 'allowed' level when i first joined. there was one team that had a django app, but they got theirs approved for special-use. over the course of the next year, working with the privacy people, the security, infrastructure, and a whole host of other teams, the RStudio suite of products got installed, Anaconda got approved, pypi got allowed, CRAN got allowed, pretty much all the infrastructure got setup and configured so that people could use R and Python. Again, i cannot take remotley full credit for the growth at the enterprise in this area, but when i started, R was not allowed, python was allowed in one project. when i left there were a couple hundred R-apps and at least 20 python apps running at any given time with dozens more in development. interns could come in and use the tools they were learning in school and directly apply their knowledge. i watched the transition of a number of pure SAS programmers to R. I wrote the setup scripts for using python in our hadoop ecosystem and know they were used by at least 30 people. i was an RStudio admin so i witnessed the explosion of R apps, from a couple developed by one person, to hundreds, developed by at least 30 people. i do not want to take credit for this growth, but i one-on-one helped at least half of those people get started (probably closer to 75%, but w/e). These are hard things to think about on a daily basis, the growth that you're being a part of. i guess its like working out, you go to the gym every day, you add a little weight or a little distance every week or two. on a day to day basis, or even a week to week basis the change is almost negligible. oo cool, i ran 3.5 miles instead of 3. oo cool, one more person setup their python environment. but when you look back at what was and now what is, [insert werblinger meme]. with those growth areas being, imo, signficant, i think they pale in comparison to the growth of version control i was part of. i had noted a number of times to my management 'what do we use for version control or collaboration' the answer was `file_v1`, `file_v2`; and `myfolder/file_v1` vs `yourfolder/file_v3`. so we got a gitlab installation setup.
personally, i only starting allowing communcations to/for me to come through gitlab. this made it so everything was timestamped and tracked, as well as anything that was said, any information would always be documented. On a side-note it also allowed everything to be async, for me, hearing that 'ding' or seeing the blinking orange light or seeing the unread email drove me crazy; having everything in gitlab meant i could get a portion of work done, check gitlab, respond to a question, edit a doc, w/e, then get back to the real work. as a builder/creator, having that freedom to be in, and stay in, the zone when i was in it was invaluable. i had a personal goal of responding to anything even semi-related to me within 24 hours. i think i met that goal, with probably 90% of things being responded to within an hour. 
the next step was getting other people/teams to start using it. i gave a few training sessions, and a few people got on board, but not many. it was extra steps for a project, more work, and the immediate benefits of a VCS aren't there all the time. i even got to lead a 2 hour training session for the entire organzation at a company retreat. I dont think i had ever done a presentation in front of hundreds of epople, at least not one in which i had to teach them things. notably i didnt have anything really prepped, i had been informed the day prior what material was wanting to be covered and i got to do the after-lunch session (with everyone with fully tummys), so, perfect :) after that day though, there was a decent uptick. soemthing like from 100 projects to 300 hundred projects. the growth rate still wasn't incredibly high, but there were more users, more projects, just more. over the next year there was someone tasked with 'expanding gitlab usage', she set out to do this with semi-formal training events occurring every week, including it as part of the orientation for new people, and showing other groups how our department was improving/collaboritng more with gitlab. i was still there to help with any technical questions (git can be hard sometimes), but i was no longer the 'champion' for it. by the time i left the number of users was 500+ (bout 80 full time active), the number of projects was around 2000, there had been 5000 issues created. again, on the day to day, not a lot of what i noticed as change, but i had setup a tracking app and looking at that one day it really struck me how far we had come as an enterprise. in order for code to be considered 'production' it now had to be in gitlab. issues were the collaboration tool between teams. teams from other orgs asked for their orgs to get gitlab bc it just looked so easy to do project management, code management, and issue tracking all in one fell swoop. again, i got the ball rolling (however slow it was going), passed it off to someone, now looking back the progress is immense. there were even half a dozen projects hooked up with CI/CD on gitlab-runners?!? CI/CD with automated tests, code-checks, builds, parameterized environments, all in an organization that about a year earlier didnt even have a standard version control. that's Gotta be progress.
