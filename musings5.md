maybe one day i'll collect all these things, edit them, and put all the linkages in (as well as many more details), or i might, in the ways of Lord T.D. and just try and get my writings published without any punctuation and random capital letters. then release an edit with a bunch of random punctuation marks stating to put them whereever you want. that sounds kinda awful, but also kinda amazing. though when i look at my last one, i see 1876 consecutive words, without a paragraph, without real spaces or any real way of breaking those thoughts and ideas into a disparate parts. in any case, this is now probably a prelude to the whole book instead of to this specific chapter, but alas, this is how james has decided to take notes/jot down ideas. makes me wish i would have taken a picture of the notes i took for an educational psychology class i took in college. notably as my 'style' contrasted with my now wife's style. she had the nice bullets, everything spaced and organized; even though her handwriting is abhorrent, things were patterned, easy to find. compared with mine which looked more like Charlie Kelly trying to sort out Pepe Silvia(https://knowyourmeme.com/memes/pepe-silvia). words written upside down connected with a squiggly line to FUN, which was circled, but in the circle were more words. the occasional bullet, but those wouldn't have the formatting of any standard bullets, there weren't (A,B,C)(1,2,3)(I,II,III)(* * *) or anything that would make sense at first glance. instead it'd be a triangle, a one, and then 3 sets of dots, but as you looked through the whole page you'd see other triangles, next to things, bc i wanted to link them all together, but notebook paper and the general population dont generally think is disjointed colors and shapes. at least thats not the way things are taught. everything is taught in terms of lines on a paper, there's graph paper to be even more precise. even in software, the most common data-choice is a 'table', or an excel sheet, something that has distinct rows and columns. All the microsoft products come with that sort of ordering. powerpoint is square slides, all in order, most of the default layouts have rows/columns laid out for you. MS Word is the same way, everything in nicely organized rows/columns. ever tried to add a non-row/column picture to word only to have it get ruined with seemingly every edit. this is businesses like things so, at least most of them. 'these are the steps we take', linearity, think on two-levels. for the majority of businesses and i think most people, that's probably enough. or at least, it has been. personally i find it very restrictive. prezi is a powerpoint alternative that i've used a couple of times that allowed the sort of interconnected ideas that i prefer, but using prezi's instead of pptx is frowned upon, bc 'this is the powerpoint template for the org. use it.'. i'd gotten the opportunity to use neo4j, a graph database, and found it to be easier for me to think of things through that format. its most common example is a friend 'graph'; friend=node, the relationship=edge. it makes finding friends of friends of friends or seeing who the most common friend is Very easy. i was asked to do a PoC with it at a previous job, but it didnt seem like many people 'got it'. im probably bad at explaining it, but the thought of everything possible all being stored/connected in one space just made so much sense to me. i always imagined taking the visio diagrams of all the interconnected systems, putting those in a graphdb. then having the systems be accessible either from the graphdb, or simply as part of it.

imagine being able to quickly and easily find everything that used the same column id; like a member or provider id in healthcare, that would be possible if the schemas are all in a graphdb, and all the systems are in there. there are a ton of metadata management solutions, one's that allow you to search everything, catalog, etc. in my mind its not a particularly hard thing though, a simple ETL script for each of the source system rdbms's auto-loads everything into the graph db. then i think you can export visio as XML, again, relatively simple parser+ETL and you're set on that end. you can even run some simple unix scripts to pull metadata from the base-fileformats: CSVs, .RData, pickle, parquet, whatever you want. or even just having file-share information available in one searchable, connected entity, makes a lot of sense to me. if adding to the graph is as easy as including an API call when your data/process is ready/finished, that makes That part super easy. with a graphdb, finding components that aren't actually used becomes easy. you can also use it as a reference to say 'oh, everyone is using personX's flat-file, but thats already in the DB, they should just use that, connect their pipeline with with a properly stored/maintained one. a significant downside to this idea is that using non-sql for many people is very difficult, you'd almost have to build a faceted UI on top of it. neo4j and the other graphdbs do come with a visual explorer, and may at this time have the ability for custom UI, but that's gonna be the differentiator. clicking on a node and seeing its connections is one thing. being able to say `select node.tablenames from graph where column.name='ID' and column_description != "the correct description"` feels like a very powerful way to find other places using the same columnname, but for a different reason. or places that are using the column but didnt properly populate the metadata. the latter is an easy fix, the former is a great conversation starter. if you can search `column.description like 'this'`, you can then start to find related columns and related systems, you can then even see if those systems are actually connected. are they duplicating data? is one 'source' better than the other(s)? do the two even know about eachother? they probably should. simple questions like that. questions that most business people dont Really care about, but they DO care about because they need the right data, at the right time, to make the right decisions. that simple mindset becomes very complicated when one does not know where the data they need is, how to get there even if they know where it is, nor are many confident with the data they get becuase there is not an easy way to know the data's qualms and caveats. one area might standardize on a three-month runout, another my use a six-month, a third might not default to any runout. all three probably have their reasons, and each might be best in a specific circumstance, but all you know is 'this table has claims, YAY, I found them!'

i think that rant ties in with the first because it is a different way of thinking about data. i've started to see the buzz and use of hadoop as a one-stop saver. because you can put anything there. but you could put anything on a normal filesystem. one place to dump all your data, even being able to access it at the same time as others or in parallel, these things are nice. but they dont solve the root problems. which is why there was the need to put hive on top of it. people needed sql. thats what they know, what they learned, what they're used to. and yes you can put 'unlimited' data there. but most companies dont have the amounts of data that the amazon and google's of the world have. notably most data scientists can do their work in R. if you're using R without connecting to an external db, the data that you're using, and the things you're doing dont need to be in hadoop. this isn't to knock R. shiny and the rstudio-products make creating/deploying an app simpler than in any other language i've tried. but i've seen many a data scientist want to 'use the big data stack' and put their data into hadoop/hive and then be sad about how slow it is, or the additional security(thanks kerberos), or bottlenecks with YARN all limit their experience. but it was an experience they didnt need to undertake. take your 1.2GB file, load it into memory call it a day. with todays processing power and the cheap costs of storage go ahead and make 10 copies of that data, it doesnt Reall matter (obviously i'd prefer if there was just one, no duplicates, and that that CSV was instead in an RDBMs, but i digress(again)). with the use of cacheing and prepopulating apps and other similar methodologies working with datasets that are less than 20 GBs doesnt necessitate hadoop. i feel like i wrote about this previously. im going to stop this one now.

to sumarize this disjointed effort that i typed multiple unrelating paragraphs. ms products and the sort of row/column thinking are the standard not because they're the best, but bc they make things simple. you ever open textedit and type , then want to do something that isn't JUST typing? not good. ms word, solves that. u wanna play with that sql'd extract ,excel nice. but ms-products should be a gateway to the better things of the world. they shouldnt be the end. just like getting your data into a table after parsing 41029412 ADT files shouldn't be the end. 
